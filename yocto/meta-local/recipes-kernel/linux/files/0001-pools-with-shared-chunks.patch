From 79ee9617859672837a8a442e034e5928382b26f8 Mon Sep 17 00:00:00 2001
From: Ioannis Galanommatis <galanom@gmail.com>
Date: Fri, 15 Dec 2017 20:51:54 +0200
Subject: [PATCH] allow pools with shared chunks

---
 lib/genalloc.c | 78 ++++++++++++++++++++++++++++++++++++++++++++--------------
 1 file changed, 59 insertions(+), 19 deletions(-)

diff --git a/lib/genalloc.c b/lib/genalloc.c
index 2b9df964c7c6..085b9ed1a56a 100644
--- a/lib/genalloc.c
+++ b/lib/genalloc.c
@@ -167,21 +167,21 @@ struct gen_pool *gen_pool_create(int min_alloc_order, int nid)
 }
 EXPORT_SYMBOL(gen_pool_create);
 
+
 /**
- * gen_pool_add_virt - add a new chunk of special memory to the pool
- * @pool: pool to add new memory chunk to
+ * gen_pool_create_chunk - create a new chunk of special memory
  * @virt: virtual starting address of memory chunk to add to pool
  * @phys: physical starting address of memory chunk to add to pool
  * @size: size in bytes of the memory chunk to add to pool
  * @nid: node id of the node the chunk structure and bitmap should be
  *       allocated on, or -1
  *
- * Add a new chunk of special memory to the specified pool.
+ * Create a new chunk of special memory
  *
- * Returns 0 on success or a -ve errno on failure.
+ * Returns the address of new chunk on success or NULL on failure.
  */
-int gen_pool_add_virt(struct gen_pool *pool, unsigned long virt, phys_addr_t phys,
-		 size_t size, int nid)
+struct gen_pool_chunk *gen_pool_create_chunk(
+	unsigned long virt, phys_addr_t phys, size_t size, int nid)
 {
 	struct gen_pool_chunk *chunk;
 	int nbits = size >> pool->min_alloc_order;
@@ -190,17 +190,54 @@ int gen_pool_add_virt(struct gen_pool *pool, unsigned long virt, phys_addr_t phy
 
 	chunk = kzalloc_node(nbytes, GFP_KERNEL, nid);
 	if (unlikely(chunk == NULL))
-		return -ENOMEM;
+		return NULL;
 
 	chunk->phys_addr = phys;
 	chunk->start_addr = virt;
 	chunk->end_addr = virt + size - 1;
 	atomic_set(&chunk->avail, size);
+	return chunk;
+}
+EXPORT_SYMBOL(gen_pool_create_chunk);
 
+
+/**
+ * gen_pool_add_chunk - add a chunk to a pool
+ * @pool: the pool where to add the chunk
+ * @chunk: the chunk to be added
+ *
+ * Adds a chunk to memory pool
+ */
+void gen_pool_add_chunk(struct gen_pool *pool, struct gen_pool_chunk *chunk)
+{
 	spin_lock(&pool->lock);
 	list_add_rcu(&chunk->next_chunk, &pool->chunks);
 	spin_unlock(&pool->lock);
+	return;
+}
+EXPORT_SYMBOL(gen_pool_add_chunk);
 
+
+/**
+ * gen_pool_add_virt - add a new chunk of special memory to the pool
+ * @pool: pool to add new memory chunk to
+ * @virt: virtual starting address of memory chunk to add to pool
+ * @phys: physical starting address of memory chunk to add to pool
+ * @size: size in bytes of the memory chunk to add to pool
+ * @nid: node id of the node the chunk structure and bitmap should be
+ *       allocated on, or -1
+ *
+ * Add a new chunk of special memory to the specified pool.
+ *
+ * Returns 0 on success or a -ve errno on failure.
+ */
+int gen_pool_add_virt(struct gen_pool *pool, unsigned long virt, phys_addr_t phys,
+		 size_t size, int nid)
+{
+	struct gen_pool_chunk *chunk = gen_pool_create_chunk(virt, phys, size, nid);
+	if (!chunk)
+		return -ENOMEM;
+	gen_pool_add_chunk(pool, chunk);
 	return 0;
 }
 EXPORT_SYMBOL(gen_pool_add_virt);
@@ -304,8 +341,6 @@ unsigned long gen_pool_alloc_algo(struct gen_pool *pool, size_t size,
 		return 0;
 
 	nbits = (size + (1UL << order) - 1) >> order;
-	rcu_read_lock();
-	//list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
 	while ((chunk = pool->chunk_algo(pool, chunk))) {
 		if (size > atomic_read(&chunk->avail))
 			continue;
@@ -329,7 +364,6 @@ unsigned long gen_pool_alloc_algo(struct gen_pool *pool, size_t size,
 		atomic_sub(size, &chunk->avail);
 		break;
 	}
-	rcu_read_unlock();
 	return addr;
 }
 EXPORT_SYMBOL(gen_pool_alloc_algo);
@@ -534,16 +568,18 @@ struct gen_pool_chunk *find_chunk_by_vaddr(struct gen_pool *pool,
 struct gen_pool_chunk *gen_chunk_first_fit(struct gen_pool *pool, 
 	struct gen_pool_chunk *last_chunk)
 {
-	if (last_chunk == NULL) 
-		return list_first_or_null_rcu(&pool->chunks,
-			struct gen_pool_chunk, next_chunk);
-	
-	if (last_chunk->next_chunk.next == &pool->chunks)
-		return NULL;
-
-	return list_next_or_null_rcu(&pool->chunks, &last_chunk->next_chunk, 
-		struct gen_pool_chunk, next_chunk);
+	struct gen_pool_chunk *chunk_sel = NULL;
 
+	rcu_read_lock();
+	if (last_chunk == NULL) {
+		chunk_sel = list_first_or_null_rcu(&pool->chunks,
+			struct gen_pool_chunk, next_chunk);
+	} else if (last_chunk->next_chunk.next != &pool->chunks) {
+		chunk_sel = list_next_or_null_rcu(&pool->chunks, &last_chunk->next_chunk, 
+			struct gen_pool_chunk, next_chunk);
+	}
+	rcu_read_unlock();
+	return chunk_sel;
 }
 EXPORT_SYMBOL_GPL(gen_chunk_first_fit);
 
@@ -566,6 +602,7 @@ struct gen_pool_chunk *gen_chunk_worst_fit(struct gen_pool *pool,
 	else avail_max = avail_last = 
 		chunk_size(last_chunk) - atomic_read(&last_chunk->avail);
 	
+	rcu_read_lock();
 	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
 		if (chunk == last_chunk) {
 			found_last = true;
@@ -585,6 +622,7 @@ struct gen_pool_chunk *gen_chunk_worst_fit(struct gen_pool *pool,
 			break;
 		}
 	}
+	rcu_read_unlock();
 	return chunk_sel;
 }
 EXPORT_SYMBOL_GPL(gen_chunk_worst_fit);
@@ -609,6 +647,7 @@ struct gen_pool_chunk *gen_chunk_best_fit(struct gen_pool *pool,
 	else avail_min = avail_last = 
 		chunk_size(last_chunk) - atomic_read(&last_chunk->avail);
 	
+	rcu_read_lock();
 	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
 		if (chunk == last_chunk) {
 			found_last = true;
@@ -625,6 +664,7 @@ struct gen_pool_chunk *gen_chunk_best_fit(struct gen_pool *pool,
 			chunk_sel = chunk;
 		}
 	}
+	rcu_read_unlock();
 
 	return chunk_sel;
 }
-- 
2.13.0

