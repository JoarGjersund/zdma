From bf9d59ce918b01174b33c5a10fc7cc89971348f9 Mon Sep 17 00:00:00 2001
From: Ioannis Galanommatis <galanom@gmail.com>
Date: Fri, 13 Oct 2017 19:26:47 +0300
Subject: [PATCH] added a chunk selection algo during alloc plus the option to
 allocate a pair from different chunks

---
 include/linux/genalloc.h |  22 +++++
 lib/genalloc.c           | 208 ++++++++++++++++++++++++++++++++++++++++++++++-
 2 files changed, 226 insertions(+), 4 deletions(-)

diff --git a/include/linux/genalloc.h b/include/linux/genalloc.h
index 29d4385903d4..19183ce83216 100644
--- a/include/linux/genalloc.h
+++ b/include/linux/genalloc.h
@@ -36,6 +36,7 @@
 struct device;
 struct device_node;
 struct gen_pool;
+//struct gen_pool_chunk;
 
 /**
  * Allocation callback function type definition
@@ -51,6 +52,10 @@ typedef unsigned long (*genpool_algo_t)(unsigned long *map,
 			unsigned int nr,
 			void *data, struct gen_pool *pool);
 
+typedef struct gen_pool_chunk *(*genpool_chunk_algo_t)
+			(struct gen_pool *pool,
+			struct gen_pool_chunk *last_chunk);
+
 /*
  *  General purpose special memory pool descriptor.
  */
@@ -60,6 +65,7 @@ struct gen_pool {
 	int min_alloc_order;		/* minimum allocation order */
 
 	genpool_algo_t algo;		/* allocation function */
+	genpool_chunk_algo_t chunk_algo;/* chunk order function */
 	void *data;
 
 	const char *name;
@@ -118,6 +124,9 @@ extern unsigned long gen_pool_alloc_algo(struct gen_pool *, size_t,
 		genpool_algo_t algo, void *data);
 extern void *gen_pool_dma_alloc(struct gen_pool *pool, size_t size,
 		dma_addr_t *dma);
+extern int gen_pool_dma_alloc_pair(struct gen_pool *pool,
+	size_t tx_size, void **tx_vaddr, dma_addr_t *tx_handle,
+	size_t rx_size, void **rx_vaddr, dma_addr_t *rx_handle);
 extern void gen_pool_free(struct gen_pool *, unsigned long, size_t);
 extern void gen_pool_for_each_chunk(struct gen_pool *,
 	void (*)(struct gen_pool *, struct gen_pool_chunk *, void *), void *);
@@ -148,6 +157,17 @@ extern unsigned long gen_pool_best_fit(unsigned long *map, unsigned long size,
 		unsigned long start, unsigned int nr, void *data,
 		struct gen_pool *pool);
 
+extern void gen_pool_set_chunk_algo(struct gen_pool *pool, 
+		genpool_chunk_algo_t algo);
+
+extern struct gen_pool_chunk *gen_chunk_first_fit(struct gen_pool *pool,
+		struct gen_pool_chunk *last_chunk);
+
+extern struct gen_pool_chunk *gen_chunk_max_avail(struct gen_pool *pool,
+		struct gen_pool_chunk *last_chunk);
+
+extern struct gen_pool_chunk *gen_chunk_least_used(struct gen_pool *pool,
+		struct gen_pool_chunk *last_chunk);
 
 extern struct gen_pool *devm_gen_pool_create(struct device *dev,
 		int min_alloc_order, int nid, const char *name);
@@ -155,6 +175,8 @@ extern struct gen_pool *gen_pool_get(struct device *dev, const char *name);
 
 bool addr_in_gen_pool(struct gen_pool *pool, unsigned long start,
 			size_t size);
+struct gen_pool_chunk *find_chunk_by_vaddr(struct gen_pool *pool, 
+	unsigned long vaddr);
 
 #ifdef CONFIG_OF
 extern struct gen_pool *of_gen_pool_get(struct device_node *np,
diff --git a/lib/genalloc.c b/lib/genalloc.c
index 144fe6b1a03e..8c854a4c96ca 100644
--- a/lib/genalloc.c
+++ b/lib/genalloc.c
@@ -159,6 +159,7 @@ struct gen_pool *gen_pool_create(int min_alloc_order, int nid)
 		INIT_LIST_HEAD(&pool->chunks);
 		pool->min_alloc_order = min_alloc_order;
 		pool->algo = gen_pool_first_fit;
+		pool->chunk_algo = gen_chunk_first_fit;
 		pool->data = NULL;
 		pool->name = NULL;
 	}
@@ -229,6 +230,7 @@ phys_addr_t gen_pool_virt_to_phys(struct gen_pool *pool, unsigned long addr)
 }
 EXPORT_SYMBOL(gen_pool_virt_to_phys);
 
+
 /**
  * gen_pool_destroy - destroy a special memory pool
  * @pool: pool to destroy
@@ -289,7 +291,7 @@ EXPORT_SYMBOL(gen_pool_alloc);
 unsigned long gen_pool_alloc_algo(struct gen_pool *pool, size_t size,
 		genpool_algo_t algo, void *data)
 {
-	struct gen_pool_chunk *chunk;
+	struct gen_pool_chunk *chunk = NULL;
 	unsigned long addr = 0;
 	int order = pool->min_alloc_order;
 	int nbits, start_bit, end_bit, remain;
@@ -303,7 +305,8 @@ unsigned long gen_pool_alloc_algo(struct gen_pool *pool, size_t size,
 
 	nbits = (size + (1UL << order) - 1) >> order;
 	rcu_read_lock();
-	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+	//list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+	while ((chunk = pool->chunk_algo(pool, chunk))) {
 		if (size > atomic_read(&chunk->avail))
 			continue;
 
@@ -321,7 +324,6 @@ unsigned long gen_pool_alloc_algo(struct gen_pool *pool, size_t size,
 			BUG_ON(remain);
 			goto retry;
 		}
-
 		addr = chunk->start_addr + ((unsigned long)start_bit << order);
 		size = nbits << order;
 		atomic_sub(size, &chunk->avail);
@@ -356,11 +358,56 @@ void *gen_pool_dma_alloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)
 
 	if (dma)
 		*dma = gen_pool_virt_to_phys(pool, vaddr);
-
+	
 	return (void *)vaddr;
 }
 EXPORT_SYMBOL(gen_pool_dma_alloc);
 
+int gen_pool_dma_alloc_pair(struct gen_pool *pool,
+	size_t size_tx, void **vaddr_tx, dma_addr_t *handle_tx,
+	size_t size_rx, void **vaddr_rx, dma_addr_t *handle_rx)
+{
+	struct gen_pool_chunk *chunk;
+	size_t avail;
+	
+	if (size_tx > size_rx) {
+		*vaddr_tx = gen_pool_dma_alloc(pool, size_tx, handle_tx);
+		chunk = find_chunk_by_vaddr(pool, (unsigned long)*vaddr_tx);
+	} else {
+		*vaddr_rx = gen_pool_dma_alloc(pool, size_rx, handle_rx);
+		chunk = find_chunk_by_vaddr(pool, (unsigned long)*vaddr_rx);
+	}
+	
+	if (chunk == NULL) return -ENOMEM; /* either allocation must have failed */
+
+	spin_lock(&pool->lock);
+	avail = atomic_read(&chunk->avail);
+	atomic_set(&chunk->avail, 0);
+	
+	if (size_tx > size_rx) {
+		*vaddr_rx = gen_pool_dma_alloc(pool, size_rx, handle_rx);
+		atomic_set(&chunk->avail, avail);
+		if (*vaddr_rx == NULL) // try to allocate from tx chunk
+			*vaddr_rx = gen_pool_dma_alloc(pool, size_rx, handle_rx);
+		if (*vaddr_rx == NULL) // if it still fails, quit...
+			gen_pool_free(pool, (unsigned long)*vaddr_tx, size_tx);
+	} else {
+		*vaddr_tx = gen_pool_dma_alloc(pool, size_tx, handle_tx);
+		atomic_set(&chunk->avail, avail);
+		if (*vaddr_tx == NULL) // as above
+			*vaddr_tx = gen_pool_dma_alloc(pool, size_tx, handle_tx);
+		if (*vaddr_tx == NULL) 
+			gen_pool_free(pool, (unsigned long)*vaddr_rx, size_rx);
+	}
+
+	spin_unlock(&pool->lock);
+
+	if (!*vaddr_tx || !*vaddr_rx) return -ENOMEM;
+	return 0;
+}
+
+EXPORT_SYMBOL(gen_pool_dma_alloc_pair);
+
 /**
  * gen_pool_free - free allocated special memory back to the pool
  * @pool: pool to free to
@@ -451,6 +498,138 @@ bool addr_in_gen_pool(struct gen_pool *pool, unsigned long start,
 	return found;
 }
 
+
+/**
+ * find_chunk_by_vaddr - finds the chunk where the address belongs
+ * @pool:	the generic memory pool
+ * @vaddr:	the virtual address to base the search
+ *
+ * Returns a pointer to the chunk which contains the specified
+ * virtual address or NULL if it is not found.
+ */
+struct gen_pool_chunk *find_chunk_by_vaddr(struct gen_pool *pool, 
+	unsigned long vaddr)
+{
+	struct gen_pool_chunk *chunk, *chunk_sel = NULL;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+		if (vaddr >= chunk->start_addr && vaddr <= chunk->end_addr) {
+			chunk_sel = chunk;
+			break;
+		}
+	}
+	rcu_read_unlock();
+	return chunk_sel;
+}
+
+/**
+ * gen_chunk_first_fit - find the immediate next of last_chunk
+ * @pool:	the memory pool
+ * @last_chunk:	the chunk after which the search will take place
+ *
+ * Returns the immidate next chunk of last_chunk or NULL if the
+ * end of the list is reached.
+ */
+struct gen_pool_chunk *gen_chunk_first_fit(struct gen_pool *pool, 
+	struct gen_pool_chunk *last_chunk)
+{
+	if (last_chunk == NULL) 
+		return list_first_or_null_rcu(&pool->chunks,
+			struct gen_pool_chunk, next_chunk);
+	
+	if (last_chunk->next_chunk.next == &pool->chunks)
+		return NULL;
+
+	return list_next_or_null_rcu(&pool->chunks, &last_chunk->next_chunk, 
+		struct gen_pool_chunk, next_chunk);
+
+}
+EXPORT_SYMBOL_GPL(gen_chunk_first_fit);
+
+/**
+ * gen_chunk_max_avail - find the next chunk with most available space
+ * @pool:	the memory pool
+ * @last_chunk:	the chunk after which the search will take place
+ *
+ * Returns the chunk after last_chunk that has the most available space
+ * but no more than of last_chunk or NULL if the end of the list is reached.
+ */
+struct gen_pool_chunk *gen_chunk_max_avail(struct gen_pool *pool, 
+	struct gen_pool_chunk *last_chunk)
+{
+	size_t avail_curr, avail_last, avail_max = 0;
+	struct gen_pool_chunk *chunk, *chunk_sel = NULL;
+	bool found_last = false;
+	
+	if (last_chunk == NULL) avail_max = 0;
+	else avail_max = avail_last = 
+		chunk_size(last_chunk) - atomic_read(&last_chunk->avail);
+	
+	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+		if (chunk == last_chunk) {
+			found_last = true;
+			continue;
+		}
+
+		avail_curr = atomic_read(&chunk->avail);
+		if (avail_curr == avail_last && found_last) {
+			chunk_sel = chunk; // first chunk that has equal available
+			break;		   // space with last_chunk
+		}
+		if (avail_curr > avail_max) {
+			avail_max = avail_curr;
+			chunk_sel = chunk;
+		}
+		if (&chunk->next_chunk == pool->chunks.prev) {
+			break;
+		}
+	}
+	return chunk_sel;
+}
+EXPORT_SYMBOL_GPL(gen_chunk_max_avail);
+
+/**
+ * gen_chunk_least_used - find the next least used chunk
+ * @pool:	the memory pool
+ * @last_chunk:	the chunk after which the search will take place
+ *
+ * Returns the chunk after last_chunk that has the least used,
+ * independently of it's total size, or NULL the end of the list
+ * is reached.
+ */
+struct gen_pool_chunk *gen_chunk_least_used(struct gen_pool *pool, 
+	struct gen_pool_chunk *last_chunk)
+{
+	size_t used_curr, used_last, used_min;
+	struct gen_pool_chunk *chunk, *chunk_sel = NULL;
+	bool found_last = false;
+	
+	if (last_chunk == NULL) used_min = -1u;
+	else used_min = used_last = 
+		chunk_size(last_chunk) - atomic_read(&last_chunk->avail);
+	
+	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+		if (chunk == last_chunk) {
+			found_last = true;
+			continue;
+		}
+
+		used_curr = chunk_size(chunk) - atomic_read(&chunk->avail);
+		if (used_curr == used_last && found_last) {
+			chunk_sel = chunk; // first chunk that has equal available
+			break;		   // space with last_chunk
+		}
+		if (used_curr < used_min) {
+			used_min = used_curr;
+			chunk_sel = chunk;
+		}
+	}
+
+	return chunk_sel;
+}
+EXPORT_SYMBOL_GPL(gen_chunk_least_used);
+
 /**
  * gen_pool_avail - get available free space of the pool
  * @pool: pool to get available free space
@@ -514,6 +693,27 @@ void gen_pool_set_algo(struct gen_pool *pool, genpool_algo_t algo, void *data)
 EXPORT_SYMBOL(gen_pool_set_algo);
 
 /**
+ * gen_pool_set_chunk_algo - set the chunk selection algorithm
+ * @pool: pool to change the chunk selection algorithm
+ * @chunk_algo: custom algorithm function
+ *
+ * The allocator calls @chunk_algo iteratively to select the next chunk 
+ * to attempt to allocate from. If @chunk_algo is NULL, the default algorithm
+ * gen_chunk_first_fit is used.
+ */
+void gen_pool_set_chunk_algo(struct gen_pool *pool, genpool_chunk_algo_t chunk_algo)
+{
+	rcu_read_lock();
+
+	pool->chunk_algo = chunk_algo;
+	if (!pool->chunk_algo)
+		pool->chunk_algo = gen_chunk_first_fit;
+	
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(gen_pool_set_chunk_algo);
+
+/**
  * gen_pool_first_fit - find the first available region
  * of memory matching the size requirement (no alignment constraint)
  * @map: The address to base the search on
-- 
2.13.0

